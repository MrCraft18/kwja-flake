special_tokens: ["<k>", "<d>", "<_>", "<dummy>"]
extended_vocab_path: ${oc.env:DATA_DIR}/typo_correction/multi_char_vocab.txt
extended_vocab_size: 342
tokenizer_kwargs:
  do_word_tokenize: false
  additional_special_tokens: ${dataset.special_tokens}
  _convert_: all
