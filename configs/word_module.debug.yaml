defaults:
  - base
  - callbacks: [early_stopping, model_checkpoint, model_summary, progress_bar]
  - datamodule: word
  - dataset: word_dataset
  - logger: wandb
  - model: word_roberta_base
  - module: default
  - optimizer: adamw
  - scheduler: constant_schedule_with_warmup
  - trainer: debug
  - _self_

devices: ${oc.env:GPUS,0}
do_predict_after_train: False

# for cohesion analysis
num_rels: 6  # TODO: fix hard coding

# set monitor and mode for early_stopping and model_checkpoint
monitor: valid/f1
mode: max
# batch size
batch_size: 4
accumulate_grad_batches: 1
